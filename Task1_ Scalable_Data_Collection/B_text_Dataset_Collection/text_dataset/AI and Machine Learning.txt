AI News is part of the TechForge Publications series TechForge Category Development March 6, 2025 Development February 26, 2025 Development February 25, 2025 Development February 24, 2025 Development February 21, 2025 Development February 19, 2025 Development February 18, 2025 Development February 18, 2025 Development February 14, 2025 Development February 11, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox All our premium content and latest tech news delivered straight to your inbox AI News is part of TechForge All our premium content and latest tech news delivered straight to your inbox

AI News is part of the TechForge Publications series TechForge Category Artificial Intelligence News. Explore the latest news and insights surrounding AI here at AI News. Artificial Intelligence March 6, 2025 Artificial Intelligence March 4, 2025 Artificial Intelligence March 3, 2025 Artificial Intelligence March 3, 2025 Artificial Intelligence February 28, 2025 Artificial Intelligence February 27, 2025 Artificial Intelligence February 26, 2025 Artificial Intelligence February 25, 2025 Artificial Intelligence February 24, 2025 Artificial Intelligence February 21, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox All our premium content and latest tech news delivered straight to your inbox AI News is part of TechForge All our premium content and latest tech news delivered straight to your inbox

AI News is part of the TechForge Publications series TechForge Category AI Enterprise News. Explore the latest news and insights on how AI is being used in enterprise here at AI News. Enterprise February 24, 2025 Enterprise February 14, 2025 Enterprise February 4, 2025 Enterprise February 4, 2025 Enterprise January 31, 2025 Enterprise January 15, 2025 Enterprise January 15, 2025 Enterprise December 17, 2024 Enterprise December 16, 2024 Enterprise December 11, 2024 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox All our premium content and latest tech news delivered straight to your inbox AI News is part of TechForge All our premium content and latest tech news delivered straight to your inbox

AI News is part of the TechForge Publications series TechForge Artificial Intelligence, Development, Enterprise, Ethics  Society, Interviews, Legislation  Government, Privacy, Security Ryan Daws February 24, 2025 Share this story Tags Categories As the AI industry focuses on transparency and security, debates around the true meaning of openness are intensifying. Experts from open-source security firm Endor Labs weighed in on these pressing topics. Andrew Stiefel, Senior Product Marketing Manager at Endor Labs, emphasised the importance of applying lessons learned from software security to AI systems. The US governments 2021 Executive Order on Improving Americas Cybersecurity includes a provision requiring organisations to produce a software bill of materials SBOM for each product sold to federal government agencies. An SBOM is essentially an inventory detailing the open-source components within a product, helping detect vulnerabilities. Stiefel argued that applying these same principles to AI systems is the logical next step. Providing better transparency for citizens and government employees not only improves security, he explained, but also gives visibility into a models datasets, training, weights, and other components. Julien Sobrier, Senior Product Manager at Endor Labs, added crucial context to the ongoing discussion about AI transparency and openness. Sobrier broke down the complexity inherent in categorising AI systems as truly open. An AI model is made of many components the training set, the weights, and programs to train and test the model, etc. It is important to make the whole chain available as open source to call the model open. It is a broad definition for now. Sobrier noted the lack of consistency across major players, which has led to confusion about the term. Among the main players, the concerns about the definition of open started with OpenAI, and Meta is in the news now for their LLAMA model even though thats more open. We need a common understanding of what an open model means. We want to watch out for any open-washing, as we saw it with free vs open-source software. One potential pitfall, Sobrier highlighted, is the increasingly common practice of open-washing, where organisations claim transparency while imposing restrictions. With cloud providers offering a paid version of open-source projects such as databases without contributing back, weve seen a shift in many open-source projects The source code is still open, but they added many commercial restrictions. Meta and other open LLM providers might go this route to keep their competitive advantage more openness about the models, but preventing competitors from using them, Sobrier warned. DeepSeek, one of the rising  albeit controversial  players in the AI industry, has taken steps to address some of these concerns by making portions of its models and code open-source. The move has been praised for advancing transparency while providing security insights. DeepSeek has already released the models and their weights as open-source, said Andrew Stiefel. This next move will provide greater transparency into their hosted services, and will give visibility into how they fine-tune and run these models in production. Such transparency has significant benefits, noted Stiefel. This will make it easier for the community to audit their systems for security risks and also for individuals and organisations to run their own versions of DeepSeek in production. Beyond security, DeepSeek also offers a roadmap on how to manage AI infrastructure at scale. From a transparency side, well see how DeepSeek is running their hosted services. This will help address security concerns that emerged after it was discovered they left some of their Clickhouse databases unsecured. Stiefel highlighted that DeepSeeks practices with tools like Docker, Kubernetes K8s, and other infrastructure-as-code IaC configurations could empower startups and hobbyists to build similar hosted instances. DeepSeeks transparency initiatives align with the broader trend toward open-source AI. A report by IDC reveals that 60 of organisations are opting for open-source AI models over commercial alternatives for their generative AI GenAI projects. Endor Labs research further indicates that organisations use, on average, between seven and twenty-one open-source models per application. The reasoning is clear leveraging the best model for specific tasks and controlling API costs. As of February 7th, Endor Labs found that more than 3,500 additional models have been trained or distilled from the original DeepSeek R1 model, said Stiefel. This shows both the energy in the open-source AI model community, and why security teams need to understand both a models lineage and its potential risks. For Sobrier, the growing adoption of open-source AI models reinforces the need to evaluate their dependencies. We need to look at AI models as major dependencies that our software depends on. Companies need to ensure they are legally allowed to use these models but also that they are safe to use in terms of operational risks and supply chain risks, just like open-source libraries. He emphasised that any risks can extend to training data They need to be confident that the datasets used for training the LLM were not poisoned or had sensitive private information. As open-source AI adoption accelerates, managing risk becomes ever more critical. Stiefel outlined a systematic approach centred around three key steps The key is finding the right balance between enabling innovation and managing risk, Stiefel said. We need to give software engineering teams latitude to experiment but must do so with full visibility. The security team needs line-of-sight and the insight to act. Sobrier further argued that the community must develop best practices for safely building and adopting AI models. A shared methodology is needed to evaluate AI models across parameters such as security, quality, operational risks, and openness. To ensure the responsible growth of AI, the industry must adopt controls that operate across several vectors Sobrier warned of complacency in the face of rapid AI progress. The community needs to build best practices to develop safe and open AI models, he advised, and a methodology to rate them along security, quality, operational risks, and openness. As Stiefel succinctly summarised Think about security across multiple vectors and implement the appropriate controls for each. See also AI in 2025 Purpose-driven models, human integration, and more Want to learn more about AI and big data from industry leaders? Check out AI  Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security  Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Ryan Daws Senior Editor March 6, 2025 March 5, 2025 March 4, 2025 March 3, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence, Machine Learning Applications, Artificial Intelligence, Companies, Enterprise, Industries, Interviews, Logistics, Manufacturing Applications, Artificial Intelligence, Enterprise, Interviews, Privacy, Security Applications, Artificial Intelligence, cloud, Interviews Applications, Artificial Intelligence, Chatbots, Companies, Privacy, Virtual Assistants March 3, 2025 Applications, Artificial Intelligence, Ethics  Society, Industries March 3, 2025 Voice Recognition March 3, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part of TechForge All our premium content and latest tech news delivered straight to your inbox

AI News is part of the TechForge Publications series TechForge AGI, Applications, Artificial Intelligence, Chatbots, Companies, Development, Ethics  Society, Virtual Assistants Ryan Daws June 24, 2024 Share this story Tags Categories SoftBank founder and CEO Masayoshi Son has claimed that artificial super intelligence ASI could be a reality within the next decade. Speaking at SoftBanks annual meeting in Tokyo on June 21, Son painted a picture of a future where AI far surpasses human intelligence, potentially revolutionising life as we know it. Son asserted that by 2030, AI could be one to 10 times smarter than humans, and by 2035, it might reach a staggering 10,000 times smarter than human intelligence. SoftBanks CEO made a clear distinction between artificial general intelligence AGI and ASI. According to Son, AGI would be equivalent to a human genius, potentially up to 10 times more capable than an average person. ASI, however, would be in a league of its own, with capabilities 10,000 times beyond human potential. Sons predictions align with the goals of Safe Superintelligence Inc. SSI, founded by Ilya Sutskever, former chief scientist at OpenAI, along with Daniel Levy and Daniel Gross. SSIs mission, as stated on their website, is to approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. The timing of these announcements underscores the growing focus on superintelligent AI within the tech industry. While SoftBank appears to be prioritising the development of ASI, SSI is emphasising the importance of safety in this pursuit. As stated by SSIs founders, We plan to advance capabilities as fast as possible while making sure our safety always remains ahead. Its worth noting that the scientific community has yet to reach a consensus on the feasibility or capabilities of AGI or ASI. Current AI systems, while impressive in specific domains, are still far from achieving human-level reasoning across all areas. Sons speech took an unexpectedly personal turn when he linked the development of ASI to his own sense of purpose and mortality. SoftBank was founded for what purpose? For what purpose was Masayoshi Son born? It may sound strange, but I think I was born to realise ASI. I am super serious about it, he declared. Sons predictions and SoftBanks apparent pivot towards ASI development, coupled with the formation of SSI, raise important questions about the future of AI and its potential impact on society. While the promise of superintelligent AI is enticing, it also brings concerns about job displacement, ethical considerations, and the potential risks associated with creating an intelligence that far surpasses our own. Whether Sons vision of ASI within a decade proves prescient or overly optimistic remains to be seen, but one thing is certain the race towards superintelligent AI is heating up, with major players positioning themselves at the forefront. See also Anthropics Claude 3.5 Sonnet beats GPT-4o in most benchmarks Want to learn more about AI and big data from industry leaders? Check out AI  Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security  Cloud Expo. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Ryan Daws Senior Editor March 6, 2025 March 5, 2025 March 4, 2025 March 3, 2025 Subscribe now to get all our premium content and latest tech news delivered straight to your inbox Artificial Intelligence, Machine Learning Applications, Artificial Intelligence, Companies, Enterprise, Industries, Interviews, Logistics, Manufacturing Applications, Artificial Intelligence, Enterprise, Interviews, Privacy, Security Applications, Artificial Intelligence, cloud, Interviews Applications, Artificial Intelligence, Chatbots, Companies, Privacy, Virtual Assistants March 3, 2025 Applications, Artificial Intelligence, Ethics  Society, Industries March 3, 2025 Voice Recognition March 3, 2025 All our premium content and latest tech news delivered straight to your inbox AI News is part of TechForge All our premium content and latest tech news delivered straight to your inbox

Back Results for  Follow IndiaAI Contribute Contact Us Home AI Standards  Policies Policies and regulatory updates in the global Artificial IntelligenceAI space. To nurture the AI ecosystem in the right direction, theres a strong need to formulate a framework for ethics and standards for other key pressing areas. Join our newsletter to know about important developments in AI space

Back Results for  Follow IndiaAI Contribute Contact Us Home Terms and Conditions IndiaAI is a web portal operated by the Ministry of Electronics and IT MeitY. It is deployed as a single window to everything AI in India. The documents and information on this website are for reference purposes only and do not purport to be legal documents. IndiaAI does not warrant the accuracy or completeness of the information, text, graphics, links or other items contained within the Website. As a result of updates and corrections, the web contents are subject to change without any notice from the IndiaAI at the IndiaAI website. In case of any variance between what has been stated and that contained in the relevant Act, Rules, Regulations, Policy Statements etc., the latter shall prevail. Any specific advice or replies to queries in any part of the website areare the personal viewsopinion of expertsconsultantspersons and are not necessarily subscribed to by IndiaAI Website. Certain links on the website lead to resources on other websites maintained by third parties over whom IndiaAI has no control or connection. These websites are external to IndiaAI, and by visiting these, you are outside the IndiaAI website and its channels. IndiaAI neither endorses in any way nor offers any judgment or warranty and accepts no responsibility or liability for the authenticity or availability of any of the goods or services or any damage, loss or harm, direct or consequential or any violation of local or international laws that may be incurred by your visiting and transacting on these websites. Join our newsletter to know about important developments in AI space

Back Results for  Follow IndiaAI Contribute Contact Us Home Explore happenings  significant occurances in Artificial IntelligenceAI. Get your content published Filter How are you evangelising AI for the society? Tell us all about your events here Join our newsletter to know about important developments in AI space

Back Results for  Follow IndiaAI Contribute Contact Us Home Research reports and Case Studies Research Report Case Studies Get your content published Discoveries  breakthroughs through fragments of reports in Artificial IntelligenceAI. INDIAai Summit Report provides a comprehensive overview of the discussions, insights, and recommendations from a summit focused on the future of Artificial Intelligence AI in India. GPAI Convening on Global Health and AI Global IndiaAI Summit, 2024 Global partnership on Artificial Intelligence GPAI Summit - 2023 The State of Responsible AI in India 2023 Join our newsletter to know about important developments in AI space

Back Results for  Follow IndiaAI Contribute Contact Us Home Datasets Curated collection of AI datasets to enhance and facilitate research initiatives Filter Domains Data Type Dataset Types Source Type Languages Join our newsletter to know about important developments in AI space

